{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL1 Data Handler Demo\n",
    "\n",
    "This notebook demonstrates how to use DL1 Data Handler to write/read IACT data for use in machine learning analysis in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Data\n",
    "\n",
    "Using DL1 Data Writer, it is easy to convert data from simtel or other data formats to a standardized HDF5 (PyTables) file format which can be used conveniently in Python. Note: The code below is very similar to the code which can be found at dl1-data-handler/scripts/write_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl1_data_handler.writer import DL1DataWriter, CTAMLDataDumper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first define a runlist which tells the Data Writer which data files to process and what output HDF5 files we want (their names, locations, and which input files they correspond to). Note that you may have to adjust the runlist below to the desired files/locations. \n",
    "\n",
    "NOTE: Runlists can also be provided from the command line in the form of a YAML file and these files can be generated automatically using the dl1-data-handler/scripts/generate_runlist.py script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runlist = [\n",
    "    {\n",
    "        'inputs': [\"dl1_data_handler_demo/gamma_20deg_0deg_run100___cta-prod3-demo_desert-2150m-Paranal-baseline-sample_cone10.simtel.gz\",\n",
    "                    \"dl1_data_handler_demo/gamma_20deg_0deg_run101___cta-prod3-demo_desert-2150m-Paranal-baseline-sample_cone10.simtel.gz\",\n",
    "                    \"dl1_data_handler_demo/gamma_20deg_0deg_run102___cta-prod3-demo_desert-2150m-Paranal-baseline-sample_cone10.simtel.gz\",\n",
    "                    \"dl1_data_handler_demo/gamma_20deg_0deg_run103___cta-prod3-demo_desert-2150m-Paranal-baseline-sample_cone10.simtel.gz\"],\n",
    "        'target': \"dl1_data_handler_demo/gamma_20deg_0deg_runs100-103___cta-prod3-demo_desert-2150m-Paranal-baseline-sample_cone10.h5\"\n",
    "    },\n",
    "     {\n",
    "        'inputs': [\"dl1_data_handler_demo/proton_20deg_0deg_run100___cta-prod3-demo_desert-2150m-Paranal-baseline-sample.simtel.gz\",\n",
    "                    \"dl1_data_handler_demo/proton_20deg_0deg_run101___cta-prod3-demo_desert-2150m-Paranal-baseline-sample.simtel.gz\",\n",
    "                    \"dl1_data_handler_demo/proton_20deg_0deg_run102___cta-prod3-demo_desert-2150m-Paranal-baseline-sample.simtel.gz\",\n",
    "                    \"dl1_data_handler_demo/proton_20deg_0deg_run103___cta-prod3-demo_desert-2150m-Paranal-baseline-sample.simtel.gz\"],\n",
    "        'target': \"dl1_data_handler_demo/proton_20deg_0deg_runs100-103___cta-prod3-demo_desert-2150m-Paranal-baseline-sample_cone10.h5\"\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "print(\"Number of input files in runlist: {}\".format(\n",
    "    len([input_file for run in runlist for input_file in run['inputs']])))\n",
    "print(\"Number of output files requested: {}\".format(\n",
    "    len(runlist)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The writing of data by DL1DataWriter is handled in two stages. First, the data is loaded from the input files into ctapipe containers using ctapipe's event source API. Any input format which is supported by ctapipe event sources (including most IACT data formats) can therefore be read in.\n",
    "\n",
    "Here we set any desired settings for the ctapipe event source we'll be using. We can select a specific subclass of eventsource if we want to, but here we don't, as in that case DL1DataWriter will simply use ctapipe.io.event_source(), which automatically chooses the correct class to use based on the input file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_src_settings = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second part of the process, dumping the data from the ctapipe containers to a specific output format, is handled by a custom class called a DL1DataDumper. \n",
    "\n",
    "An implementation of DL1DataDumper designed to output PyTables HDF5 files in the standard \"CTA ML format\" is already provided. It is called CTAMLDataDumper. However, if you would like to implement an alternative data format, it is as easy as implementing your own Data Dumper class inheriting from the generic DL1DataDumper. Here we select CTAMLDataDumper as our desired dumper and designate some settings related to the data dumping. Most of these settings are used for optimizing the output HDF5 files (i.e. compressing them efficiently, adding indexes on columns, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dumper_class = CTAMLDataDumper\n",
    "\n",
    "dumper_settings = {\n",
    "    'filter_settings': {\n",
    "        'complib': 'lzo',\n",
    "        'complevel': 1\n",
    "    },\n",
    "    'expected_tel_types': 10,\n",
    "    'expected_tels': 300,\n",
    "    'expected_events': 10000,\n",
    "    'expected_images_per_event': {\n",
    "        'LST:LSTCam': 0.5,\n",
    "        'MST:NectarCam': 2.0,\n",
    "        'MST:FlashCam': 2.0,\n",
    "        'MST-SCT:SCTCam': 1.5,\n",
    "        'SST:DigiCam': 1.25,\n",
    "        'SST:ASTRICam': 1.25,\n",
    "        'SST:CHEC': 1.25\n",
    "    },\n",
    "    'index_columns': [\n",
    "        ['/Events', 'mc_energy'],\n",
    "        ['/Events', 'alt'],\n",
    "        ['/Events', 'az'],\n",
    "        ['tel', 'event_index']\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we set some general settings relating to the writer as a whole. Note that all of these settings can also be specified, like the runlist, using an external YAML config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer_settings = {\n",
    "    'calibration_settings': {\n",
    "       'r1_product': 'HESSIOR1Calibrator',\n",
    "       'extractor_product': 'NeighbourPeakIntegrator'\n",
    "    },\n",
    "    'output_file_size': 1073741824,\n",
    "    'events_per_file': 1000\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all that remains to do is to instantiate our DataWriter and then call process_data with our runlist. After a brief wait, the output files we requested in our runlist should be written!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_writer = DL1DataWriter(event_source_class=None,\n",
    "                            event_source_settings=event_src_settings,\n",
    "                            data_dumper_class=data_dumper_class,\n",
    "                            data_dumper_settings=dumper_settings,\n",
    "                            **writer_settings)\n",
    "\n",
    "data_writer.process_data(runlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can verify that the output files were created and see their sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for run in runlist:\n",
    "    size = os.path.getsize(run['target'])\n",
    "    print(\"File: {}, Size: {}\".format(run['target'], size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data\n",
    "\n",
    "Having created some sample data files, we can now use DL1DataReader to read examples from them and examine their contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl1_data_handler.reader import DL1DataReader\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading from a data file is as easy as creating a reader and passing in the path to the HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reader = DL1DataReader([\"dl1_data_handler_demo/gamma_20deg_0deg_runs100-103___cta-prod3-demo_desert-2150m-Paranal-baseline-sample_cone10.h5\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DL1DataReader internally uses ImageMapper to map from the raw traces contained in the HDF5 files to a 2D NumPy array. This format is much more convenient for use with various computer vision, ML, or deep learning libraries for image analysis. For square pixel cameras the conversion to a 2D array is trivial, but for hexagonal pixel cameras an implementation of oversampling, interpolation, or rebinning must be used. By default, ImageMapper uses oversampling, but all 3 approaches (and variants of them) are fully implemented. \n",
    "\n",
    "We can see the shape of one example mapped LST image here (note that the LST camera has hexagonal pixels, so oversampling was used to process the raw trace):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Image shape: {}\".format(reader[0][0].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DL1DataReader can read data in a number of modes. Here we did not pass any specific settings when constructing our reader so it uses its default mode, \"mono\", where each image is outputted separately as a single example. In this case we are ignoring which images came from the same event/shower and simply treating them as independent examples. Here is the description of the reader's output, which as you can see consists of a single LST image per example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reader.example_description)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily plot some of these mapped images to verify that the data was written, read, and transformed correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_IMAGES_TO_PLOT = 5\n",
    "\n",
    "i = 0\n",
    "while i < NUM_IMAGES_TO_PLOT:\n",
    "    example = reader[i]\n",
    "    image = example[0]       \n",
    "    plt.figure()\n",
    "    plt.pcolor(image[:,:,0],cmap='viridis')\n",
    "    plt.axes().set_aspect('equal')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more about the full functionality of DL1DataReader, as well as DL1DataWriter and the other components of DL1DataHandler, check out the repository at: https://github.com/cta-observatory/dl1-data-handler. It is still under development, so please contact us about any feature requests or ideas for using it in your own projects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
