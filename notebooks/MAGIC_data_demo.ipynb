{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Data\n",
    "\n",
    "This notebook demonstrates how to use DL1 Data Handler to write/read MAGIC data for use in machine learning analysis in Python. ctapipe-extra need to be installed on top of dl1-data-handler to read MAGIC data from ROOT files. Please do \"conda install ctapipe-extra\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl1_data_handler.writer import DL1DataWriter, CTAMLDataDumper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please change the path to your directory hosting the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/tmiener/deeplearning/magic_school/\"\n",
    "\n",
    "calibrated_root_file = data_dir + \"GA_M*_za05to35_8_821319_Y_w0.root\"\n",
    "superstar_root_file = data_dir + \"GA_za05to35_8_821319_S_w0.root\"\n",
    "\n",
    "calibrated_hdf5_file = data_dir + \"GA_za05to35_8_821319_Y_w0_dl1dh_v0.9.0.h5\"\n",
    "superstar_hdf5_file = data_dir + \"GA_za05to35_8_821319_S_w0_dl1dh_v0.9.0.h5\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a walk-through with example files. For a dataset production, please set up the runlist automatically using scripts/generate_runlist.py and execute on the command line:\n",
    "\n",
    "```bash\n",
    "python scripts/write_data.py [runlist] [--config_file CONFIG_FILE_PATH] [--output_dir OUTPUT_DIR] [--debug]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "runlist_calibrated = [\n",
    "    {\n",
    "        'inputs': [calibrated_root_file],\n",
    "        'target': calibrated_hdf5_file\n",
    "    },\n",
    "]\n",
    "\n",
    "# If you would like to use the DL1DH with superstar files,\n",
    "# make sure you had run \"star\" with flags \"-saveimages -saveimagesclean -savecerevt\".\n",
    "# i.e.:\n",
    "# $ star -b -f -mc -q -saveimages -saveimagesclean -savecerevt --config=mrcfiles/star_M{1,2}_OSA.rc --ind=\"/home/tjark/MAGIC_files/cal/*M{1,2}*_Y_*.root\" --out=\"/home/tjark/MAGIC_files/starM{1,2}/\" --log=/home/tjark/MAGIC_files/starM{1,2}/LogFile.txt\n",
    "# $ superstar -q -b -f -mc --config=mrcfiles/superstar.rc --ind1=/home/tjark/MAGIC_files/starM1/GA_M1_za05to35_8_*_I_w0.root --ind2=/home/tjark/MAGIC_files/starM2/GA_M2_za05to35_8_*_I_w0.root --out=/home/tjark/MAGIC_files/superstar/ --log=/home/tjark/MAGIC_files/superstar/logfile.txt\n",
    "runlist_superstar = [\n",
    "    {\n",
    "        'inputs': [superstar_root_file],\n",
    "        'target': superstar_hdf5_file\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "print(\"Number of input files in runlist: {}\".format(\n",
    "    len([input_file for run in runlist_calibrated for input_file in run['inputs']])))\n",
    "print(\"Number of output files requested: {}\".format(\n",
    "    len(runlist_calibrated)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, for each event, data is read from the ROOT files in the DL1DataWriter. Then, this data is sent to the CTAMLDataDumper to dump it to a HDF5 file with the DL1DH data format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_src_settings = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dumper_class = CTAMLDataDumper\n",
    "\n",
    "dumper_settings = {\n",
    "    'filter_settings': {\n",
    "        'complib': 'lzo',\n",
    "        'complevel': 1\n",
    "    },\n",
    "    'expected_tel_types': 1,\n",
    "    'expected_tels': 2,\n",
    "    'expected_events': 300,\n",
    "    'expected_images_per_event': {\n",
    "        'MAGIC:MAGICCam': 2.0\n",
    "    },\n",
    "    'index_columns': [\n",
    "        ['/Events', 'mc_energy'],\n",
    "        ['/Events', 'alt'],\n",
    "        ['/Events', 'az'],\n",
    "        ['tel', 'event_index']\n",
    "    ],\n",
    "    # Various hillas parameters caculation via ctapipe are stored additionally in the same file.\n",
    "    'cleaning_settings': [\n",
    "        {'algorithm': 'tailcuts_clean',\n",
    "        'args': {\n",
    "            'picture_thresh': 10,\n",
    "            'boundary_thresh': 5}},\n",
    "        \n",
    "        {'algorithm': 'tailcuts_clean',\n",
    "        'args': {\n",
    "            'picture_thresh': 20,\n",
    "            'boundary_thresh': 5}}\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic writer settinngs\n",
    "writer_settings = {\n",
    "    # Parallel for doing everything via python multiprocessing\n",
    "    'write_mode': 'serial',\n",
    "    'output_file_size': 1378500,\n",
    "    'events_per_file': 300,\n",
    "    # Selected the telescopes, which should be dumped into the hdf5. M1 and M2 for MAGIC.\n",
    "    'selected_telescope_ids': [1, 2],\n",
    "    # This is used for the main hillas parameter table.\n",
    "    # If the code is called with the superstar files,\n",
    "    # it will ignore this dict and write the MARS\n",
    "    # hillas parameters to the main parameter table.\n",
    "    'cleaning_settings': {\n",
    "        'algorithm': 'tailcuts_clean',\n",
    "        'args': {\n",
    "            'picture_thresh': 6,\n",
    "            'boundary_thresh': 3}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we instantiate our DL1DataWriter and then call process_data with our runlist. After a brief wait, the output files we requested in our runlist should be written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_writer = DL1DataWriter(event_source_class=None,\n",
    "                            event_source_settings=event_src_settings,\n",
    "                            data_dumper_class=data_dumper_class,\n",
    "                            data_dumper_settings=dumper_settings,\n",
    "                            write_mode = writer_settings['write_mode'],\n",
    "                            output_file_size = writer_settings['output_file_size'],\n",
    "                            events_per_file = writer_settings['events_per_file'],\n",
    "                            selected_telescope_ids = writer_settings['selected_telescope_ids'],\n",
    "                            cleaning_settings = writer_settings['cleaning_settings'],\n",
    "                        )\n",
    "\n",
    "data_writer.process_data(runlist_calibrated)\n",
    "data_writer.process_data(runlist_superstar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can cross check the size of the output files created now. The MAGIC superstar were produced using the standard input_cards, so there are the MAGIC standard cuts applied, i.e. the file size of the superstar DL1DH hdf5 should be much smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for run in runlist_calibrated:\n",
    "    size = os.path.getsize(run['target'])\n",
    "    print(\"File: {}, Size: {}\".format(run['target'], size))\n",
    "    \n",
    "\n",
    "for run in runlist_superstar:\n",
    "    size = os.path.getsize(run['target'])\n",
    "    print(\"File: {}, Size: {}\".format(run['target'], size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have created the HDF5 files, we can easily use DL1DataReader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl1_data_handler.reader import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a reader by passing the path to HDF5 file. Hence, using ImageMapper, we can plot the gamma images and verify the transformation is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "parameter_selection = [{'col_name': \"hillas_intensity\", 'min_value':1000.0}]\n",
    "selection_string = '(mc_energy > 1.0)'\n",
    "reader = DL1DataReaderDL1DH([calibrated_hdf5_file],\n",
    "                            mode='mono',\n",
    "                            selected_telescope_types=[\"LST_MAGIC_MAGICCam\"],\n",
    "                            selection_string = selection_string,\n",
    "                            parameter_selection = parameter_selection,\n",
    "                            image_channels = ['charge'])\n",
    "NUM_IMAGES_TO_PLOT = 10\n",
    "i = 0\n",
    "while i < NUM_IMAGES_TO_PLOT:\n",
    "    example = reader[i]\n",
    "    image = example[0]\n",
    "    plt.figure()\n",
    "    plt.pcolor(image[:,:,0],cmap='viridis')\n",
    "    plt.axes().set_aspect('equal')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "reader = DL1DataReaderDL1DH([superstar_hdf5_file],\n",
    "                            mode='stereo',\n",
    "                            #selected_telescope_types=[\"LST_MAGIC_MAGICCam\"],\n",
    "                            #selected_telescope_ids = {\"LST_MAGIC_MAGICCam\": [1]},\n",
    "                            image_channels = ['charge', 'peak_time', 'image_mask'],\n",
    "                            parameter_list = ['hillas_intensity', 'hillas_x'],\n",
    "                            event_info = [\"mc_energy\", \"alt\", \"az\"])\n",
    "\n",
    "NUM_IMAGES_TO_PLOT = 10\n",
    "i = 0\n",
    "while i < NUM_IMAGES_TO_PLOT:\n",
    "    print(\"Event nr. {}\".format(i+1))\n",
    "    example = reader[i]\n",
    "    image = example[1]\n",
    "    plt.figure()\n",
    "    plt.pcolor(image[0,:,:,0],cmap='viridis')\n",
    "    plt.axes().set_aspect('equal')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    plt.pcolor(image[1,:,:,0],cmap='viridis')\n",
    "    plt.axes().set_aspect('equal')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    plt.pcolor(image[0,:,:,1],cmap='viridis')\n",
    "    plt.axes().set_aspect('equal')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    plt.pcolor(image[1,:,:,1],cmap='viridis')\n",
    "    plt.axes().set_aspect('equal')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    plt.pcolor(image[0,:,:,2],cmap='viridis')\n",
    "    plt.axes().set_aspect('equal')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    plt.pcolor(image[1,:,:,2],cmap='viridis')\n",
    "    plt.axes().set_aspect('equal')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
